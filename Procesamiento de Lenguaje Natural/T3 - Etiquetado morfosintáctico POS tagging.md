# Etiquetado morfosintÃ¡ctico (POS tagging)

### Contenido
- [x] CategorÃ­as morfosintÃ¡cticas o gramaticales
- [x] Funcionamiento y caracterÃ­sticas del etiquetado morfosintÃ¡ctico
- [x] Etiquetado morfosintÃ¡ctico basado en modelos ocultos de Markov (HMM)
- [ ] Etiquetado morfosintÃ¡ctico basado en aprendizaje automÃ¡tico

---------

![Menu](./assets/T3Screenshot_1.png)

Hablaremos del etiquetado morfosintÃ¡ctico y cÃ³mo calcularlo, haciendo especial hincapiÃ© en los modelos  ocultos de Markov (Hidden Markov Models)

## CategorÃ­as morfosintÃ¡cticas o gramaticales
**Morfosintaxis** :  Parte de la gramÃ¡tica que integra la **morfologÃ­a** y la **sintaxis**

Las categorÃ­as morfosintÃ¡cticas del lenguaje, que en espaÃ±ol tambiÃ©n se llaman categorÃ­as gramaticales, proporcionan una clasificaciÃ³n de las diferentes partes de la oraciÃ³n, es decir, una clasificaciÃ³n de las palabras segÃºn su tipo.

Las categorÃ­as gramaticales del espaÃ±ol, segÃºn la clasificaciÃ³n clÃ¡sica (RAE), son 9:


| CategorÃ­as  morfosintÃ¡cticas| Detalle |
|---------------------|----------------------------------------------|
|**Sustantivo o nombre**| Clase de palabras cuyos elementos poseen gÃ©nero y nÃºmero, forman sintagmas nominales con diversas funciones sintÃ¡cticas y designan entidades de diferente naturaleza.|
|  **Determinante**     |  Clase de palabras cuyos elementos determinan al sustantivo o al grupo nominal y se sitÃºan generalmente en posiciÃ³n prenominal. El artÃ­culo definido y los demostrativos son determinantes. |
|  **Adjetivo**          |  Clase de palabras cuyos elementos modifican a un sustantivo o se predican de Ã©l y denotan cualidades, propiedades y relaciones de diversa naturaleza. Ejemplos: inteligente, amplio, numÃ©rico, mismo, telefÃ³nicoâ€¦ |
|  **Pronombre**  | Clase de palabras cuyos elementos hacen las veces del sustantivo o del sintagma nominal y que se emplean para referirse a las personas, los animales o las cosas sin nombrarlos. P. ej.: ella, esto, quiÃ©nâ€¦ |
|  **Verbo**   | Clase de palabras cuyos elementos pueden tener variaciÃ³n de persona, nÃºmero, tiempo, modo y aspecto. |
| **Adverbio** |  Clase de palabras cuyos elementos son invariables y tÃ³nicos, estÃ¡n dotados generalmente de significado lÃ©xico y modifican el significado de varias categorÃ­as, principalmente de un verbo, de un adjetivo, de una oraciÃ³n o de una palabra de la misma clase. |
| **PreposiciÃ³n** | Clase de palabras invariables cuyos elementos se caracterizan por introducir un tÃ©rmino, generalmente nominal u oracional, con el que forman grupo sintÃ¡ctico |
| **ConjunciÃ³n**| Clase de palabras invariables, generalmente Ã¡tonas, cuyos elementos manifiestan relaciones de coordinaciÃ³n o subordinaciÃ³n entre palabras, grupos sintÃ¡cticos u oraciones. |
|**InterjecciÃ³n** | Clase de palabras invariables, con cuyos elementos se forman enunciados exclamativos, que manifiestan impresiones, verbalizan sentimientos o realizan actos de habla apelativos. |


## Funcionamiento y caracterÃ­sticas del etiquetado morfosintÃ¡ctico
El etiquetado morfosintÃ¡ctico, llamado **POS tagging** (part-of-speech tagging) en inglÃ©s, es el proceso para identificar las diferentes partes de la oraciÃ³n y consiste en asignar una etiqueta (tag) sobre la categorÃ­a gramatical a cada una de las palabras de un texto de entrada.

La entrada del algoritmo de etiquetado morfosintÃ¡ctico es una secuencia de palabras y la salida del algoritmo es una secuencia de pares formados por la palabra y la correspondiente etiqueta indicando la categorÃ­a gramatical a la que pertenece dicha palabra.

Hoy en dÃ­a, la mayorÃ­a de los algoritmos de procesamiento del lenguaje natural que procesan palabras en inglÃ©s utilizan el **Penn Treebank** (Marcus, Santorini y Marcinkiewicz, 1993).

![Menu](./assets/T3Screenshot_3.png)

#### Ejemplos. 
Si el etiquetador morfosintÃ¡ctico analiza la frase **"bebo un vaso del vino tinto"**, suponiendo que se utilizan las etiquetas para las categorÃ­as gramaticales definidas en el Penn Treebank, la salida serÃ­a:

`bebo/VBP un/DT vaso/NN de/IN el/DT vino/NN tinto/JJ`

Para la frase **"vino de un lugar lejano"**, el etiquetado morfosintÃ¡ctico serÃ­a:

`vino/VBZ de/IN un/DT lugar/NN lejano/JJ`

AsÃ­, el etiquetado morfosintÃ¡ctico realiza durante su funcionamiento un proceso de **desambiguaciÃ³n**: una palabra, que es ambigua y puede pertenecer a mÃ¡s de una categorÃ­a gramatical, se etiqueta correctamente segÃºn el contexto de la frase analizada.

Identificar que una palabra es una contracciÃ³n (del) y separarla en las dos (de y el) que la constituyen forma parte del proceso previo de **preprocesado** de la oraciÃ³n.

La identificaciÃ³n de las palabras de una oraciÃ³n tambiÃ©n es llamada proceso de **obtenciÃ³n de los tokens,** porque token es el nombre inglÃ©s para definir una cadena de caracteres que representa una palabra y se realiza siempre previamente al etiquetado morfosintÃ¡ctico y a otras tareas de procesamiento del lenguaje natural.


## Etiquetado morfosintÃ¡ctico basado en modelos ocultos de Markov (HMM)
Un modelo oculto de Markov es un **modelo estadÃ­stico** que se puede representar como una mÃ¡quina de estados finitos, pero **donde las transacciones entre estados son probabilÃ­sticas** y no determinÃ­sticas. El objetivo es determinar los parÃ¡metros desconocidos (ocultos) a partir de los parÃ¡metros observables.

Para el etiquetado morfosintÃ¡ctico, los HMM son entrenados en un conjunto de datos totalmente etiquetados, los HMM fijan estimaciones de mÃ¡xima verosimilitud para cada uno de los estados y determina las diferentes probabilidades que rigen el modelo. Para llevar a cabo el proceso de estimaciÃ³n de probabilidades se usa el algoritmo de decodificaciÃ³n de Viterbi (Forney, 1973). 

El objetivo de decodificaciÃ³n HMM es elegir la secuencia de etiquetas mÃ¡s probable dada la secuencia de observaciÃ³n de ğ‘› palabras ğ‘¤ ğ‘›1:

EcuaciÃ³n Final, agregando la regla de bayes y simplificando al eliminar el denominador:

![Menu](./assets/T3Screenshot_2.png)

Los **etiquetadores HMM** hacen dos suposiciones que permiten simplificar estas ecuaciones aÃºn mÃ¡s:
1. La primera es que la probabilidad de apariciÃ³n de una palabra depende solo de su 
propia etiqueta y es independiente de las palabras y etiquetas vecinas.
2. La segunda suposiciÃ³n, tambiÃ©n llamada bigrama o digrama, es que la probabilidad de una etiqueta solo depende de la etiqueta anterior, en lugar de toda la secuencia de etiquetas.

Aplicando estas suposiciones a las ecuaciones anteriores terminamos con la siguiente ecuaciÃ³n para la secuencia de etiquetas mÃ¡s probable de un etiquetador bigrama, las cuales corresponden a la **probabilidad de emisiÃ³n** y la **probabilidad de transiciÃ³n** de un HMM:

![Menu](./assets/T3Screenshot_4.png)

### Ejemplos de probabilidad de transiciÃ³n y probabilidad de emisiÃ³n
> Un **corpus lingÃ¼Ã­stico** es una colecciÃ³n de textos representativos de una lengua que se utilizan para el anÃ¡lisis lingÃ¼Ã­stico. Los corpus pueden estar **anotados o etiquetados** de forma que las palabras que lo conforman presentan, ademÃ¡s, algÃºn tipo de informaciÃ³n lingÃ¼Ã­stica.

#### **Probabilidades de transiciÃ³n**
Las probabilidades de transiciÃ³n de etiqueta ğ‘ƒ(ğ‘¡ğ‘–|ğ‘¡ğ‘–âˆ’1) representan la probabilidad de una etiqueta dada la etiqueta anterior. Por ejemplo, los verbos modales (etiqueta MD) como Â«canÂ» (poder) son muy probablemente seguidos por un verbo en la forma base (etiqueta VB) como Â«runÂ» (correr), por lo que espera que esta probabilidad sea 
alta.

En el corpus WSJ, por ejemplo, los MD aparecen 13 124 veces, de las cuales son seguidos por un VB 10 471 veces, lo cual resulta en una estimaciÃ³n de mÃ¡xima probabilidad de 80% :

![Menu](./assets/T3Screenshot_5.png)

#### **Probabilidades de emisiÃ³n**
Las probabilidades de emisiÃ³n ğ‘ƒ(ğ‘¤1ğ‘›|ğ‘¡1ğ‘›) representan la probabilidad de que, dada una etiqueta (digamos MD), esta se asocie con una palabra concreta (digamos Â«willÂ»). La estimaciÃ³n de **mÃ¡xima verosimilitud **de la probabilidad de emisiÃ³n en general se define como:

![Menu](./assets/T3Screenshot_6.png)

De los 13 124 casos de MD en el corpus WSJ, estas se asocian o refieren a Â«willÂ» en 
4046 ocasiones, por tanto:

![Menu](./assets/T3Screenshot_7.png)

Como aclaraciÃ³n, hemos de decir que esta probabilidad no se refiere a Â«cuÃ¡l es la etiqueta mÃ¡s probable para la palabra â€˜willâ€™Â», ya que esta serÃ­a la probabilidad a posteriori ğ‘ƒ(ğ‘€ğ·|ğ‘¤ğ‘–ğ‘™ğ‘™). En su lugar, ğ‘ƒ(ğ‘¤ğ‘–ğ‘™ğ‘™|ğ‘€ğ·) responde a una pregunta ligeramente menos intuitiva, concretamente **Â«si vamos a generar una etiqueta MD, Â¿quÃ© probabilidades hay de que este MD sea â€˜wilâ€™?Â»**. ğŸ˜€

La figura 2 ilustra algunas de las probabilidades de transiciÃ³n A para tres estados en un etiquetador morfosintÃ¡ctico HMM; el etiquetador completo tendrÃ­a un estado para cada etiqueta. En esta imagen, las probabilidades de transiciÃ³n A se utilizan para calcular la probabilidad a priori

![Menu](./assets/T3Screenshot_8.png)

La figura 3 muestra otra vista de estos tres estados, pero centrÃ¡ndose en algunas de las probabilidades de observaciÃ³n B de cada palabra. Cada estado oculto estÃ¡ asociado con un vector de probabilidades para cada palabra en observaciÃ³n.

![Menu](./assets/T3Screenshot_9.png)


#### Ejemplo. 
Finalmente, vamos a trabajar a travÃ©s de un ejemplo de cÃ¡lculo de la mejor secuencia de etiquetas que corresponde a la siguiente secuencia de palabras: **Â«Janet will back the billÂ»** (en espaÃ±ol, Â«Janet respaldarÃ¡ la leyÂ»). La secuencia de etiquetas correcta es:

`Janet/NNP will/MD back/VB the/DT bill/NN`

Sea el modelo HMM el definido por el conjunto A de probabilidades de transiciÃ³n (figura 4), y el conjunto B de probabilidades de observaciÃ³n (figura 5). Cada elemento ğ‘ğ‘–ğ‘— del conjunto A describe la probabilidad de transitar de un estado oculto ğ‘– (etiqueta ğ‘–) a otro estado oculto ğ‘— (etiqueta ğ‘—). Cada elemento ğ‘ğ‘–(ğ‘œğ‘¡) describe la probabilidad de observar las palabras dadas las etiquetas:

![Menu](./assets/T3Screenshot_10.png)

La siguiente imagen (figura 5) se obtiene a partir del recuento de apariciones de una palabra en el corpus. AsÃ­:
- La palabra Janet solo aparece como un nombre propio (NNP)
- La palabra will aparece en el corpus como tres categorÃ­as gramaticales diferentes; puede ser:
  - Un verbo modal (MD) para generar el futuro de los verbos en inglÃ©s.
  - Un verbo (VB) que significa Â«desearÂ» en espaÃ±ol.
  - O un nombre (NN) que significa Â«deseoÂ» o Â«voluntadÂ» en espaÃ±ol

![Menu](./assets/T3Screenshot_11.png)


La figura 6 muestra un esquema con las posibles etiquetas para el ejemplo anterior, asÃ­ como la correcta secuencia de etiquetado final. Esta secuencia del etiquetado morfosintÃ¡ctico se ha calculado aplicando el algoritmo de Viterbi, cuyo pseudocÃ³digo se presenta en la figura 7.

![Menu](./assets/T3Screenshot_12.png)

![Menu](./assets/T3Screenshot_13.png)

El algoritmo de Viterbi crea una matriz de probabilidades con una columna para** cada observaciÃ³n** ğ‘¡ y una fila para** cada estado** ğ‘ğ‘– de la **mÃ¡quina de estados finitos **(o autÃ³mata finito) que representa el HMM. Para el ejemplo, el algoritmo crea N = 5 columnas de estado, la primera para la observaciÃ³n de la primera palabra Â«JanetÂ», la segunda para Â«willÂ» y asÃ­ sucesivamente hasta completar las cinco palabras que conforman la frase, tal como se muestra en la figura 8.

![Menu](./assets/T3Screenshot_14.png)
